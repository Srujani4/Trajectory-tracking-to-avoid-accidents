{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "6fRNXjOCECXg",
        "outputId": "f38b8413-aaf4-4a76-d7e0-48f9d3bada04"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cpu\n",
            "0.0    13410\n",
            "1.0     3420\n",
            "Name: interesting_agent, dtype: int64\n",
            "       case_id  track_id  frame_id  timestamp_ms agent_type         x  \\\n",
            "81         1.0         8         1           100        car  1060.560   \n",
            "82         1.0         8         2           200        car  1059.611   \n",
            "83         1.0         8         3           300        car  1058.662   \n",
            "84         1.0         8         4           400        car  1057.712   \n",
            "85         1.0         8         5           500        car  1056.762   \n",
            "...        ...       ...       ...           ...        ...       ...   \n",
            "24272    342.0         6         6           600        car  1035.409   \n",
            "24273    342.0         6         7           700        car  1034.923   \n",
            "24274    342.0         6         8           800        car  1034.435   \n",
            "24275    342.0         6         9           900        car  1033.943   \n",
            "24276    342.0         6        10          1000        car  1033.449   \n",
            "\n",
            "             y     vx     vy  psi_rad  length  width  track_to_predict  \\\n",
            "81     976.187 -9.510  5.544    2.614    4.47   1.81               1.0   \n",
            "82     976.745 -9.515  5.586    2.611    4.47   1.81               1.0   \n",
            "83     977.307 -9.520  5.631    2.607    4.47   1.81               1.0   \n",
            "84     977.874 -9.523  5.676    2.604    4.47   1.81               1.0   \n",
            "85     978.446 -9.524  5.722    2.601    4.47   1.81               1.0   \n",
            "...        ...    ...    ...      ...     ...    ...               ...   \n",
            "24272  993.997 -4.927  3.927    2.469    4.81   1.93               1.0   \n",
            "24273  994.399 -4.966  4.029    2.460    4.81   1.93               1.0   \n",
            "24274  994.812 -5.004  4.140    2.450    4.81   1.93               1.0   \n",
            "24275  995.237 -5.042  4.259    2.440    4.81   1.93               1.0   \n",
            "24276  995.674 -5.079  4.387    2.429    4.81   1.93               1.0   \n",
            "\n",
            "       interesting_agent  \n",
            "81                   1.0  \n",
            "82                   1.0  \n",
            "83                   1.0  \n",
            "84                   1.0  \n",
            "85                   1.0  \n",
            "...                  ...  \n",
            "24272                1.0  \n",
            "24273                1.0  \n",
            "24274                1.0  \n",
            "24275                1.0  \n",
            "24276                1.0  \n",
            "\n",
            "[3420 rows x 14 columns]\n",
            "   case_id  track_id  frame_id  timestamp_ms agent_type         x         y  \\\n",
            "0      1.0         1         1           100        car   990.866  1042.230   \n",
            "1      1.0         1         2           200        car   990.085  1043.155   \n",
            "2      1.0         1         3           300        car   989.304  1044.080   \n",
            "3      1.0         2         1           100        car   990.717  1037.215   \n",
            "4      1.0         2         2           200        car   989.944  1038.163   \n",
            "5      1.0         2         3           300        car   989.172  1039.113   \n",
            "6      1.0         2         4           400        car   988.403  1040.064   \n",
            "7      1.0         2         5           500        car   987.635  1041.017   \n",
            "8      1.0         2         6           600        car   986.869  1041.972   \n",
            "9      1.0         3         1           100        car  1006.473  1024.324   \n",
            "\n",
            "      vx     vy  psi_rad  length  width  track_to_predict  interesting_agent  \n",
            "0 -7.830  9.233    2.274    4.23   1.86               0.0                0.0  \n",
            "1 -7.817  9.242    2.273    4.23   1.86               0.0                0.0  \n",
            "2 -7.805  9.253    2.272    4.23   1.86               0.0                0.0  \n",
            "3 -7.769  9.452    2.259    5.88   2.40               0.0                0.0  \n",
            "4 -7.747  9.470    2.256    5.88   2.40               0.0                0.0  \n",
            "5 -7.726  9.488    2.254    5.88   2.40               0.0                0.0  \n",
            "6 -7.707  9.504    2.252    5.88   2.40               0.0                0.0  \n",
            "7 -7.688  9.520    2.250    5.88   2.40               0.0                0.0  \n",
            "8 -7.668  9.536    2.248    5.88   2.40               0.0                0.0  \n",
            "9 -8.653  8.565    2.361    4.45   1.88               0.0                0.0  \n",
            "0.0    13410\n",
            "1.0     3420\n",
            "Name: interesting_agent, dtype: int64\n",
            "       case_id  track_id  frame_id  timestamp_ms agent_type         x  \\\n",
            "81         1.0         8         1           100        car  1060.560   \n",
            "82         1.0         8         2           200        car  1059.611   \n",
            "83         1.0         8         3           300        car  1058.662   \n",
            "84         1.0         8         4           400        car  1057.712   \n",
            "85         1.0         8         5           500        car  1056.762   \n",
            "...        ...       ...       ...           ...        ...       ...   \n",
            "24272    342.0         6         6           600        car  1035.409   \n",
            "24273    342.0         6         7           700        car  1034.923   \n",
            "24274    342.0         6         8           800        car  1034.435   \n",
            "24275    342.0         6         9           900        car  1033.943   \n",
            "24276    342.0         6        10          1000        car  1033.449   \n",
            "\n",
            "             y     vx     vy  psi_rad  length  width  track_to_predict  \\\n",
            "81     976.187 -9.510  5.544    2.614    4.47   1.81               1.0   \n",
            "82     976.745 -9.515  5.586    2.611    4.47   1.81               1.0   \n",
            "83     977.307 -9.520  5.631    2.607    4.47   1.81               1.0   \n",
            "84     977.874 -9.523  5.676    2.604    4.47   1.81               1.0   \n",
            "85     978.446 -9.524  5.722    2.601    4.47   1.81               1.0   \n",
            "...        ...    ...    ...      ...     ...    ...               ...   \n",
            "24272  993.997 -4.927  3.927    2.469    4.81   1.93               1.0   \n",
            "24273  994.399 -4.966  4.029    2.460    4.81   1.93               1.0   \n",
            "24274  994.812 -5.004  4.140    2.450    4.81   1.93               1.0   \n",
            "24275  995.237 -5.042  4.259    2.440    4.81   1.93               1.0   \n",
            "24276  995.674 -5.079  4.387    2.429    4.81   1.93               1.0   \n",
            "\n",
            "       interesting_agent  \n",
            "81                   1.0  \n",
            "82                   1.0  \n",
            "83                   1.0  \n",
            "84                   1.0  \n",
            "85                   1.0  \n",
            "...                  ...  \n",
            "24272                1.0  \n",
            "24273                1.0  \n",
            "24274                1.0  \n",
            "24275                1.0  \n",
            "24276                1.0  \n",
            "\n",
            "[3420 rows x 14 columns]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-5-8b4dc9db24d0>:292: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  dataset[\"x\"] = dataset[\"x\"] * PIXEL_SCALE\n",
            "<ipython-input-5-8b4dc9db24d0>:293: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  dataset[\"y\"] = dataset[\"y\"] * PIXEL_SCALE\n",
            "<ipython-input-5-8b4dc9db24d0>:296: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  dataset[\"x\"] = (dataset[\"x\"] - dataset[\"x\"].mean()) / dataset[\"x\"].std()\n",
            "<ipython-input-5-8b4dc9db24d0>:297: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  dataset[\"y\"] = (dataset[\"y\"] - dataset[\"y\"].mean()) / dataset[\"y\"].std()\n",
            "<ipython-input-5-8b4dc9db24d0>:298: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  dataset[\"vx\"] = (dataset[\"vx\"] - dataset[\"vx\"].mean()) / dataset[\"vx\"].std()\n",
            "<ipython-input-5-8b4dc9db24d0>:299: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  dataset[\"vy\"] = (dataset[\"vy\"] - dataset[\"vy\"].mean()) / dataset[\"vy\"].std()\n",
            "<ipython-input-5-8b4dc9db24d0>:300: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  dataset[\"psi_rad\"] = (\n",
            "<ipython-input-5-8b4dc9db24d0>:303: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  dataset[\"length\"] = (\n",
            "<ipython-input-5-8b4dc9db24d0>:306: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  dataset[\"width\"] = (dataset[\"width\"] - dataset[\"width\"].mean()) / dataset[\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-8b4dc9db24d0>\u001b[0m in \u001b[0;36m<cell line: 450>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    474\u001b[0m     \u001b[0miterations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m4000\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    475\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 476\u001b[0;31m     \u001b[0mavg_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mavg_reward_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdouble_dqn_cnn_object\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    477\u001b[0m     \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime_string\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"/double_dqn_cnn_loss.npy\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mavg_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    478\u001b[0m     \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime_string\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"/double_dqn_cnn_avg_reward.npy\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mavg_reward_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-5-8b4dc9db24d0>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, num_epochs, dataset)\u001b[0m\n\u001b[1;32m    338\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    339\u001b[0m                 \u001b[0mstep\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 340\u001b[0;31m                 \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoose_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    341\u001b[0m                 \u001b[0mstate_next\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    342\u001b[0m                 exp = {\n",
            "\u001b[0;32m<ipython-input-5-8b4dc9db24d0>\u001b[0m in \u001b[0;36mchoose_action\u001b[0;34m(self, state, epsilon)\u001b[0m\n\u001b[1;32m    259\u001b[0m         \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    260\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 261\u001b[0;31m             \u001b[0maction_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mestimate_net\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    262\u001b[0m             \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction_value\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    263\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-5-8b4dc9db24d0>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, state)\u001b[0m\n\u001b[1;32m    190\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m       \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 192\u001b[0;31m       \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    193\u001b[0m       \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m       \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (1x5 and 11x128)"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from typing import Dict, List, Text\n",
        "\n",
        "import os\n",
        "import copy\n",
        "import random\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# import tensorflow\n",
        "# from tensorflow.keras.models import Sequential\n",
        "# from tensorflow.keras.layers import Dense, Flatten\n",
        "# from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "from typing import Tuple\n",
        "from highway_env import utils\n",
        "from highway_env.envs.common.abstract import AbstractEnv\n",
        "from highway_env.envs.common.action import Action\n",
        "from highway_env.road.road import Road, RoadNetwork\n",
        "from highway_env.vehicle.controller import ControlledVehicle\n",
        "from highway_env.vehicle.kinematics import Vehicle\n",
        "from highway_env.vehicle.behavior import IDMVehicle\n",
        "from highway_env.vehicle.kinematics import Vehicle\n",
        "\n",
        "Observation = np.ndarray\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)\n",
        "\n",
        "dataset = pd.read_csv(\"//content/DR_LaneChange_ET0_obs.csv\")\n",
        "\n",
        "class AbstractEnv:\n",
        "    @classmethod\n",
        "    def default_config(cls) -> Dict:\n",
        "        return {}\n",
        "\n",
        "\n",
        "class HighwayEnv(AbstractEnv):\n",
        "    def __init__(self, dataset: pd.DataFrame, config: Dict):\n",
        "        self.dataset = dataset\n",
        "        self.agent_id = 0  # ID of the controlled agent (ego agent)\n",
        "        self.controlled_vehicle = None  # The controlled vehicle (ego agent)\n",
        "        self.config = config\n",
        "        self.np_random = np.random.RandomState()\n",
        "\n",
        "\n",
        "    @classmethod\n",
        "    def default_config(cls) -> Dict:\n",
        "        config = super().default_config()\n",
        "        config.update({\n",
        "            \"observation\": {\n",
        "                \"type\": \"Kinematics\"\n",
        "            },\n",
        "            \"action\": {\n",
        "                \"type\": \"DiscreteMetaAction\",\n",
        "            },\n",
        "            \"lanes_count\": 4,\n",
        "            \"vehicles_count\": 50,\n",
        "            \"controlled_vehicles\": 1,\n",
        "            \"initial_lane_id\": None,\n",
        "            \"duration\": 40,  # [s]\n",
        "            \"ego_spacing\": 2,\n",
        "            \"vehicles_density\": 1,\n",
        "            \"collision_reward\": -1,\n",
        "            \"right_lane_reward\": 0.1,\n",
        "            \"high_speed_reward\": 0.4,\n",
        "            \"lane_change_reward\": 0,\n",
        "            \"reward_speed_range\": [20, 30],\n",
        "            \"normalize_reward\": True,\n",
        "            \"offroad_terminal\": False,\n",
        "            \"show_trajectories\": False  # Add this attribute\n",
        "        })\n",
        "        return config\n",
        "\n",
        "    def _reset(self) -> Observation:\n",
        "        self._create_road()\n",
        "        self._create_vehicles()\n",
        "        state = np.array([\n",
        "            self.controlled_vehicle.position[0],\n",
        "            self.controlled_vehicle.position[1],\n",
        "            self.controlled_vehicle.velocity[0],\n",
        "            self.controlled_vehicle.velocity[1],\n",
        "            self.controlled_vehicle.heading\n",
        "        ])\n",
        "        state = np.expand_dims(state, axis=0)  # Add an extra dimension for the batch\n",
        "        state = np.expand_dims(state, axis=0)  # Add an extra dimension for the number of channels\n",
        "        return state\n",
        "\n",
        "    def _step(self, action: Action) -> Tuple[Observation, float, bool, Dict]:\n",
        "        # Perform the given action in the environment and get the next state, reward, done flag, and info\n",
        "        next_state, reward, done, info = self.controlled_vehicle.step(action)\n",
        "\n",
        "        # Preprocess the next state\n",
        "        next_state = self._preprocess_state(next_state)\n",
        "\n",
        "        return next_state, reward, done, info\n",
        "\n",
        "    def step(self, action: Action) -> Tuple[Observation, float, bool, Dict]:\n",
        "        return self._step(action)\n",
        "\n",
        "\n",
        "    def _create_road(self) -> None:\n",
        "        \"\"\"Create a road composed of straight adjacent lanes.\"\"\"\n",
        "        self.road = Road(network=RoadNetwork.straight_road_network(self.config[\"lanes_count\"], speed_limit=30),\n",
        "                         np_random=self.np_random, record_history=False)\n",
        "\n",
        "    def _create_vehicles(self) -> None:\n",
        "        \"\"\"Create vehicles from the dataset.\"\"\"\n",
        "        # Extract controlled vehicle data from the dataset\n",
        "        self.dataset = pd.read_csv(\"/content/DR_LaneChange_ET0_obs.csv\")\n",
        "\n",
        "        self.agent_id = 1  # ID of the controlled agent (ego agent)\n",
        "\n",
        "        agent_data = self.dataset[self.dataset[\"track_to_predict\"] == self.agent_id]\n",
        "        print(agent_data['interesting_agent'].value_counts())\n",
        "\n",
        "        # Initialize the controlled vehicle (ego agent)\n",
        "        controlled_vehicle_data = agent_data[agent_data[\"interesting_agent\"] == 1]\n",
        "        print(controlled_vehicle_data)\n",
        "        x, y, vx, vy, psi_rad, length, width = controlled_vehicle_data.iloc[0][[\"x\", \"y\", \"vx\", \"vy\", \"psi_rad\", \"length\", \"width\"]]\n",
        "        self.controlled_vehicle = ControlledVehicle(\n",
        "        road=self.road,\n",
        "        position=[x, y],\n",
        "        heading=psi_rad,\n",
        "        )\n",
        "        self.road.vehicles.append(self.controlled_vehicle)\n",
        "\n",
        "        # Create non-controlled vehicles (other agents) from the dataset\n",
        "        other_vehicles_data = agent_data.loc[agent_data[\"interesting_agent\"] == 0]\n",
        "        for _, vehicle_data in other_vehicles_data.iterrows():\n",
        "            x, y, vx, vy, psi_rad, length, width = vehicle_data[[\"x\", \"y\", \"vx\", \"vy\", \"psi_rad\", \"length\", \"width\"]]\n",
        "            vehicle = IDMVehicle(\n",
        "              road=self.road,\n",
        "              position=[x, y],\n",
        "              speed=np.linalg.norm([vx, vy]),\n",
        "              heading=psi_rad,\n",
        "    # more parameters here if IDMVehicle requires them...\n",
        "              )\n",
        "\n",
        "            self.road.vehicles.append(vehicle)\n",
        "\n",
        "\n",
        "    def _reward(self, action: Action) -> float:\n",
        "        \"\"\"\n",
        "        The reward is defined to foster driving at high speed, on the rightmost lanes, and to avoid collisions.\n",
        "        :param action: the last action performed\n",
        "        :return: the corresponding reward\n",
        "        \"\"\"\n",
        "        rewards = self._rewards(action)\n",
        "        reward = sum(self.config.get(name, 0) * reward for name, reward in rewards.items())\n",
        "        if self.config[\"normalize_reward\"]:\n",
        "            reward = utils.lmap(reward,\n",
        "                                [self.config[\"collision_reward\"],\n",
        "                                 self.config[\"high_speed_reward\"] + self.config[\"right_lane_reward\"]],\n",
        "                                [0, 1])\n",
        "        reward *= rewards['on_road_reward']\n",
        "        return reward\n",
        "\n",
        "    def _rewards(self, action: Action) -> Dict[Text, float]:\n",
        "        neighbours = self.road.network.all_side_lanes(self.controlled_vehicle.lane_index)\n",
        "        lane = self.controlled_vehicle.target_lane_index[2]\n",
        "        # Use forward speed rather than speed, see https://github.com/eleurent/highway-env/issues/268\n",
        "        forward_speed = self.controlled_vehicle.speed * np.cos(self.controlled_vehicle.heading)\n",
        "        scaled_speed = utils.lmap(forward_speed, self.config[\"reward_speed_range\"], [0, 1])\n",
        "        return {\n",
        "            \"collision_reward\": float(self.controlled_vehicle.crashed),\n",
        "            \"right_lane_reward\": lane / max(len(neighbours) - 1, 1),\n",
        "            \"high_speed_reward\": np.clip(scaled_speed, 0, 1),\n",
        "            \"on_road_reward\": float(self.controlled_vehicle.on_road)\n",
        "        }\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class Net(nn.Module):\n",
        "    def __init__(self, state_dim, action_dim):\n",
        "      super(Net, self).__init__()\n",
        "      hidden_nodes1 = 128\n",
        "      hidden_nodes2 = 64\n",
        "      self.flatten= nn.Flatten()\n",
        "      self.fc1 = nn.Linear(state_dim, hidden_nodes1)\n",
        "      self.fc2 = nn.Linear(hidden_nodes1, hidden_nodes2)\n",
        "      self.fc3 = nn.Linear(hidden_nodes2, action_dim)\n",
        "\n",
        "    def forward(self, state):\n",
        "      x = self.flatten(state)\n",
        "      x = F.relu(self.fc1(state))\n",
        "      x = F.relu(self.fc2(x))\n",
        "      out = self.fc3(x)\n",
        "      return out\n",
        "\n",
        "# def build_model(states, actions):\n",
        "#     model = Sequential()\n",
        "#     model.add(Dense(24, activation='relu', input_shape=states))\n",
        "#     model.add(Dense(24, activation='relu'))\n",
        "#     model.add(Dense(actions, activation='linear'))\n",
        "#     return model\n",
        "\n",
        "# model = build_model(states, actions)\n",
        "\n",
        "# model.summary()\n",
        "\n",
        "\n",
        "class DOUBLEDQN_CNN(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        env,\n",
        "        state_dim,\n",
        "        action_dim,\n",
        "        lr=0.001,\n",
        "        gamma=0.99,\n",
        "        batch_size=5,\n",
        "        timestamp=\"\",\n",
        "    ):\n",
        "        \"\"\"\n",
        "        :param env: object, a gym environment\n",
        "        :param state_dim: int, size of state space\n",
        "        :param action_dim: int, size of action space\n",
        "        :param lr: float, learning rate\n",
        "        :param gamma: float, discount factor\n",
        "        :param batch_size: int, batch size for training\n",
        "        \"\"\"\n",
        "        super(DOUBLEDQN_CNN, self).__init__()\n",
        "\n",
        "        self.env = env\n",
        "        # self.env = HighwayEnv(dataset, config={})  # Instantiate HighwayEnv with the dataset\n",
        "        self.env._reset()\n",
        "        self.timestamp = timestamp\n",
        "\n",
        "        self.test_env = copy.deepcopy(env)  # for evaluation purpose\n",
        "        self.state_dim = state_dim\n",
        "        self.action_dim = action_dim\n",
        "        self.gamma = gamma\n",
        "        self.batch_size = batch_size\n",
        "        self.learn_step_counter = 0\n",
        "\n",
        "        # Pass the dataset to the HighwayEnv class\n",
        "        # self.highway_env = HighwayEnv(dataset, config={})  # Adjust the config as needed\n",
        "\n",
        "        self.target_net = Net(self.state_dim, self.action_dim).to(device)\n",
        "        self.estimate_net = Net(self.state_dim, self.action_dim).to(device)\n",
        "        # self.ReplayBuffer = Replay(1000, 100, self.state_dim, self.action_dim)\n",
        "\n",
        "        self.optimizer = torch.optim.Adam(self.estimate_net.parameters(), lr=lr)\n",
        "\n",
        "    def update_target_networks(self):\n",
        "        \"\"\"\n",
        "        A function to update the target networks\n",
        "        \"\"\"\n",
        "        self.target_net.load_state_dict(self.estimate_net.state_dict())\n",
        "    def choose_action(self, state, epsilon=0.9):\n",
        "        state = np.expand_dims(state, axis=0)\n",
        "        state = np.expand_dims(state, axis=0)\n",
        "        state = torch.FloatTensor(state).to(device)\n",
        "        if np.random.randn() <= epsilon:\n",
        "            action_value = self.estimate_net(state)\n",
        "            action = torch.argmax(action_value).item()\n",
        "        else:\n",
        "            action = np.random.randint(0, self.action_dim)\n",
        "        return action\n",
        "    def preprocess_dataset(self, dataset):\n",
        "        \"\"\"\n",
        "        Preprocess the dataset to make it compatible with the highway_env\n",
        "\n",
        "        :param dataset: pandas DataFrame, input dataset\n",
        "        :return: numpy array, preprocessed dataset\n",
        "        \"\"\"\n",
        "        # Extract relevant columns\n",
        "        PIXEL_SCALE = 4\n",
        "        dataset = dataset[\n",
        "            [\n",
        "                \"frame_id\",\n",
        "                \"timestamp_ms\",\n",
        "                \"x\",\n",
        "                \"y\",\n",
        "                \"vx\",\n",
        "                \"vy\",\n",
        "                \"psi_rad\",\n",
        "                \"length\",\n",
        "                \"width\",\n",
        "                \"agent_type\",\n",
        "                \"interesting_agent\",\n",
        "            ]\n",
        "        ]\n",
        "\n",
        "        # Convert positions from meters to pixels\n",
        "        dataset[\"x\"] = dataset[\"x\"] * PIXEL_SCALE\n",
        "        dataset[\"y\"] = dataset[\"y\"] * PIXEL_SCALE\n",
        "\n",
        "        # Normalize positions, velocities, yaw angle, length, and width\n",
        "        dataset[\"x\"] = (dataset[\"x\"] - dataset[\"x\"].mean()) / dataset[\"x\"].std()\n",
        "        dataset[\"y\"] = (dataset[\"y\"] - dataset[\"y\"].mean()) / dataset[\"y\"].std()\n",
        "        dataset[\"vx\"] = (dataset[\"vx\"] - dataset[\"vx\"].mean()) / dataset[\"vx\"].std()\n",
        "        dataset[\"vy\"] = (dataset[\"vy\"] - dataset[\"vy\"].mean()) / dataset[\"vy\"].std()\n",
        "        dataset[\"psi_rad\"] = (\n",
        "            dataset[\"psi_rad\"] - dataset[\"psi_rad\"].mean()\n",
        "        ) / dataset[\"psi_rad\"].std()\n",
        "        dataset[\"length\"] = (\n",
        "            dataset[\"length\"] - dataset[\"length\"].mean()\n",
        "        ) / dataset[\"length\"].std()\n",
        "        dataset[\"width\"] = (dataset[\"width\"] - dataset[\"width\"].mean()) / dataset[\n",
        "            \"width\"\n",
        "        ].std()\n",
        "        # Encode agent types using one-hot encoding\n",
        "        agent_types = pd.get_dummies(dataset[\"agent_type\"], prefix=\"agent_type\")\n",
        "        dataset = pd.concat([dataset, agent_types], axis=1)\n",
        "\n",
        "        dataset.drop(columns=[\"agent_type\"], inplace=True)\n",
        "\n",
        "        return dataset.to_numpy()\n",
        "\n",
        "    def train(self, num_epochs, dataset):\n",
        "        \"\"\"\n",
        "        Train the policy for the given number of iterations\n",
        "\n",
        "        :param num_epochs: int, number of epochs to train the policy for\n",
        "        :param dataset: pandas DataFrame, input dataset\n",
        "        :return: list, training loss\n",
        "        \"\"\"\n",
        "        dataset = self.preprocess_dataset(dataset)\n",
        "\n",
        "        count_list = []\n",
        "        loss_list = []\n",
        "        total_reward_list = []\n",
        "        avg_reward_list = []\n",
        "        epoch_reward = 0\n",
        "\n",
        "        state = self.env._reset()  # Reset the environment once at the beginning of training\n",
        "        for epoch in range(int(num_epochs)):\n",
        "            done = False\n",
        "            avg_loss = 0\n",
        "            step = 0\n",
        "            while not done:\n",
        "                step += 1\n",
        "                action = self.choose_action(state)\n",
        "                state_next, reward, done, _ = self.env.step(action)\n",
        "                exp = {\n",
        "                    \"state\": state,\n",
        "                    \"action\": action,\n",
        "                    \"reward\": reward,\n",
        "                    \"state_next\": state_next,\n",
        "                    \"done\": done,\n",
        "                }\n",
        "                self.ReplayBuffer.buffer_add(exp)\n",
        "                state = state_next\n",
        "\n",
        "                # sample random batch from replay memory\n",
        "                exp_batch = self.ReplayBuffer.buffer_sample(self.batch_size)\n",
        "\n",
        "                # extract batch data\n",
        "                state_batch = torch.FloatTensor([exp[\"state\"] for exp in exp_batch])\n",
        "                action_batch = torch.LongTensor([exp[\"action\"] for exp in exp_batch])\n",
        "                reward_batch = torch.FloatTensor([exp[\"reward\"] for exp in exp_batch])\n",
        "                state_next_batch = torch.FloatTensor(\n",
        "                    [exp[\"state_next\"] for exp in exp_batch]\n",
        "                )\n",
        "                done_batch = torch.FloatTensor([1 - exp[\"done\"] for exp in exp_batch])\n",
        "\n",
        "                # reshape\n",
        "                state_batch = state_batch.to(device).unsqueeze(1)\n",
        "                action_batch = action_batch.to(device).unsqueeze(1)\n",
        "                reward_batch = reward_batch.to(device).unsqueeze(1)\n",
        "                state_next_batch = state_next_batch.to(device).unsqueeze(1)\n",
        "                done_batch = done_batch.to(device).unsqueeze(1)\n",
        "\n",
        "                # get estimate Q value\n",
        "                estimate_Q = self.estimate_net(state_batch).gather(1, action_batch)\n",
        "\n",
        "                # get target Q value\n",
        "                max_action_idx = self.estimate_net(state_next_batch).detach().argmax(1)\n",
        "                target_Q = reward_batch + done_batch * self.gamma * self.target_net(\n",
        "                    state_next_batch\n",
        "                ).gather(1, max_action_idx.unsqueeze(1))\n",
        "\n",
        "                # compute mse loss\n",
        "                loss = F.mse_loss(estimate_Q, target_Q)\n",
        "                avg_loss += loss.item()\n",
        "\n",
        "                # update network\n",
        "                self.optimizer.zero_grad()\n",
        "                loss.backward()\n",
        "                self.optimizer.step()\n",
        "\n",
        "                # update target network\n",
        "                if self.learn_step_counter % 100 == 0:\n",
        "                    self.update_target_networks()\n",
        "                self.learn_step_counter += 1\n",
        "\n",
        "            reward, count = self.eval()\n",
        "            epoch_reward += reward\n",
        "\n",
        "            # save\n",
        "            period = 40\n",
        "            if epoch % period == 0:\n",
        "                # log\n",
        "                avg_loss /= step\n",
        "                epoch_reward /= period\n",
        "                avg_reward_list.append(epoch_reward)\n",
        "                loss_list.append(avg_loss)\n",
        "\n",
        "                print(\n",
        "                    \"\\nepoch: [{}/{}], \\tavg loss: {:.4f}, \\tavg reward: {:.3f}, \\tsteps: {}\".format(\n",
        "                        epoch + 1, num_epochs, avg_loss, epoch_reward, count\n",
        "                    )\n",
        "                )\n",
        "\n",
        "                epoch_reward = 0\n",
        "                # create a new directory for saving\n",
        "                try:\n",
        "                    os.makedirs(self.timestamp)\n",
        "                except OSError:\n",
        "                    pass\n",
        "                np.save(self.timestamp + \"/double_dqn_cnn_loss.npy\", loss_list)\n",
        "                np.save(\n",
        "                    self.timestamp + \"/double_dqn_cnn_avg_reward.npy\", avg_reward_list\n",
        "                )\n",
        "                torch.save(\n",
        "                    self.estimate_net.state_dict(),\n",
        "                    self.timestamp + \"/double_dqn_cnn.pkl\",\n",
        "                )\n",
        "\n",
        "        self.env.close()\n",
        "        return loss_list, avg_reward_list\n",
        "\n",
        "\n",
        "    def eval(self):\n",
        "        \"\"\"\n",
        "        Evaluate the policy\n",
        "        \"\"\"\n",
        "        count = 0\n",
        "        total_reward = 0\n",
        "        done = False\n",
        "        state = self.test_env.reset()\n",
        "\n",
        "        while not done:\n",
        "            action = self.choose_action(state, epsilon=1)\n",
        "            state_next, reward, done, _ = self.test_env.step(action)\n",
        "            total_reward += reward\n",
        "            count += 1\n",
        "            state = state_next\n",
        "\n",
        "        return total_reward, count\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Read the dataset from the CSV file\n",
        "    dataset = pd.read_csv(\"/content/DR_LaneChange_ET0_obs.csv\")\n",
        "\n",
        "    # Timestamp for saving\n",
        "    named_tuple = time.localtime()  # get struct_time\n",
        "    time_string = time.strftime(\n",
        "        \"%m%d_%H_%M\", named_tuple\n",
        "    )  # have a folder of \"date+time ex: 1209_20_36 -> December 12th, 20:36\"\n",
        "\n",
        "    myobjec=HighwayEnv(dataset = dataset, config={\"lanes_count\": 4})\n",
        "    double_dqn_cnn_object = DOUBLEDQN_CNN(\n",
        "        # HighwayEnv(dataset = hdataset, config={}),\n",
        "        myobjec,\n",
        "        state_dim=11,  # Adjust state dimension according to your dataset\n",
        "        action_dim=4,  # Adjust action dimension according to your problem\n",
        "        lr=0.001,\n",
        "        gamma=0.99,\n",
        "        batch_size=64,\n",
        "        timestamp=time_string,\n",
        "    )\n",
        "\n",
        "\n",
        "    # Train the policy\n",
        "    iterations = 4000\n",
        "    print(dataset[0:10])\n",
        "    avg_loss, avg_reward_list = double_dqn_cnn_object.train(iterations, dataset)\n",
        "    np.save(time_string + \"/double_dqn_cnn_loss.npy\", avg_loss)\n",
        "    np.save(time_string + \"/double_dqn_cnn_avg_reward.npy\", avg_reward_list)\n",
        "\n",
        "    # Save the DQN network\n",
        "    torch.save(\n",
        "        double_dqn_cnn_object.estimate_net.state_dict(),\n",
        "        time_string + \"/double_dqn_cnn.pkl\",\n",
        "    )\n",
        "\n",
        "    # Plot\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(avg_loss)\n",
        "    plt.grid()\n",
        "    plt.title(\"Double DQN Loss\")\n",
        "    plt.xlabel(\"Epochs\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "    plt.savefig(\"double_dqn_loss.png\", dpi=150)\n",
        "    plt.show()\n",
        "\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(avg_reward_list)\n",
        "    plt.grid()\n",
        "    plt.title(\"Double DQN Training Reward\")\n",
        "    plt.xlabel(\"*40 Epochs\")\n",
        "    plt.ylabel(\"Reward\")\n",
        "    plt.savefig(time_string + \"/double_dqn_cnn_train_reward.png\", dpi=150)\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install highway_env"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eHC-x_4rHh7Z",
        "outputId": "c48ac2eb-6844-4956-d982-a3c69e117877"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting highway_env\n",
            "  Downloading highway_env-1.8.2-py3-none-any.whl (104 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.0/104.0 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting gymnasium>=0.27 (from highway_env)\n",
            "  Downloading gymnasium-0.29.0-py3-none-any.whl (953 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m953.8/953.8 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from highway_env) (1.22.4)\n",
            "Requirement already satisfied: pygame>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from highway_env) (2.5.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from highway_env) (3.7.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from highway_env) (1.5.3)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from highway_env) (1.10.1)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium>=0.27->highway_env) (2.2.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium>=0.27->highway_env) (4.7.1)\n",
            "Collecting farama-notifications>=0.0.1 (from gymnasium>=0.27->highway_env)\n",
            "  Downloading Farama_Notifications-0.0.4-py3-none-any.whl (2.5 kB)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->highway_env) (1.1.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->highway_env) (0.11.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->highway_env) (4.41.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->highway_env) (1.4.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->highway_env) (23.1)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->highway_env) (8.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->highway_env) (3.1.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->highway_env) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->highway_env) (2022.7.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib->highway_env) (1.16.0)\n",
            "Installing collected packages: farama-notifications, gymnasium, highway_env\n",
            "Successfully installed farama-notifications-0.0.4 gymnasium-0.29.0 highway_env-1.8.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from typing import Dict, List, Text\n",
        "\n",
        "import os\n",
        "import copy\n",
        "import random\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from typing import Tuple\n",
        "from highway_env import utils\n",
        "from highway_env.envs.common.abstract import AbstractEnv  # Update import statement\n",
        "from highway_env.envs.common.action import Action\n",
        "from highway_env.road.road import Road, RoadNetwork\n",
        "from highway_env.vehicle.controller import ControlledVehicle\n",
        "from highway_env.vehicle.behavior import IDMVehicle\n",
        "\n",
        "Observation = np.ndarray\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)\n",
        "\n",
        "dataset = pd.read_csv(\"//content/DR_LaneChange_ET0_obs.csv\")\n",
        "\n",
        "class HighwayEnv(AbstractEnv):\n",
        "    def __init__(self, dataset: pd.DataFrame, config: Dict):\n",
        "        self.dataset = dataset\n",
        "        self.agent_id = 0  # ID of the controlled agent (ego agent)\n",
        "        self.controlled_vehicle = None  # The controlled vehicle (ego agent)\n",
        "        self.config = config\n",
        "        self.np_random = np.random.RandomState()\n",
        "        self.config.setdefault(\"lanes_count\", 4)\n",
        "\n",
        "    @classmethod\n",
        "    def default_config(cls) -> Dict:\n",
        "        config = super().default_config()\n",
        "        config.update({\n",
        "            \"lanes_count\": 4,  # Add this line to include the missing key\n",
        "            \"observation\": {\n",
        "                \"type\": \"Kinematics\"\n",
        "            },\n",
        "            \"action\": {\n",
        "                \"type\": \"DiscreteMetaAction\",\n",
        "            },\n",
        "            \"vehicles_count\": 50,\n",
        "            \"controlled_vehicles\": 1,\n",
        "            \"initial_lane_id\": None,\n",
        "            \"duration\": 40,  # [s]\n",
        "            \"ego_spacing\": 2,\n",
        "            \"vehicles_density\": 1,\n",
        "            \"collision_reward\": -1,\n",
        "            \"right_lane_reward\": 0.1,\n",
        "            \"high_speed_reward\": 0.4,\n",
        "            \"lane_change_reward\": 0,\n",
        "            \"reward_speed_range\": [20, 30],\n",
        "            \"normalize_reward\": True,\n",
        "            \"offroad_terminal\": False,\n",
        "            \"show_trajectories\": False  # Add this attribute\n",
        "        })\n",
        "        return config\n",
        "\n",
        "    def _reset(self) -> Observation:\n",
        "        self._create_road()\n",
        "        self._create_vehicles()\n",
        "        state = np.array([\n",
        "            self.controlled_vehicle.position[0],\n",
        "            self.controlled_vehicle.position[1],\n",
        "            self.controlled_vehicle.velocity[0],\n",
        "            self.controlled_vehicle.velocity[1],\n",
        "            self.controlled_vehicle.heading\n",
        "        ])\n",
        "        state = np.expand_dims(state, axis=0)  # Add an extra dimension for the batch\n",
        "        state = np.expand_dims(state, axis=0)  # Add an extra dimension for the number of channels\n",
        "        return state\n",
        "\n",
        "    def _step(self, action: Action) -> Tuple[Observation, float, bool, Dict]:\n",
        "        # Perform the given action in the environment and get the next state, reward, done flag, and info\n",
        "        next_state, reward, done, info = self.controlled_vehicle.step(action)\n",
        "\n",
        "        # Preprocess the next state\n",
        "        next_state = self._preprocess_state(next_state)\n",
        "\n",
        "        return next_state, reward, done, info\n",
        "\n",
        "    def step(self, action: Action) -> Tuple[Observation, float, bool, Dict]:\n",
        "        return self._step(action)\n",
        "\n",
        "\n",
        "    def _create_road(self) -> None:\n",
        "        \"\"\"Create a road composed of straight adjacent lanes.\"\"\"\n",
        "        # self.road = Road(network=RoadNetwork.straight_road_network(self.config[\"lanes_count\"], speed_limit=30),\n",
        "        #                  np_random=self.np_random, record_history=False)\n",
        "        lanes_count = self.config.get(\"lanes_count\", 4)\n",
        "        self.road = Road(\n",
        "            network=RoadNetwork.straight_road_network(lanes_count, speed_limit=30),\n",
        "            np_random=self.np_random,\n",
        "            record_history=False,\n",
        "        )\n",
        "\n",
        "    def _create_vehicles(self) -> None:\n",
        "        \"\"\"Create vehicles from the dataset.\"\"\"\n",
        "        # Extract controlled vehicle data from the dataset\n",
        "        self.agent_id = 1  # ID of the controlled agent (ego agent)\n",
        "        agent_data = self.dataset[self.dataset[\"track_to_predict\"] == self.agent_id]\n",
        "\n",
        "        # Initialize the controlled vehicle (ego agent)\n",
        "        controlled_vehicle_data = agent_data[agent_data[\"interesting_agent\"] == 1]\n",
        "        x, y, vx, vy, psi_rad, length, width = controlled_vehicle_data.iloc[0][\n",
        "            [\"x\", \"y\", \"vx\", \"vy\", \"psi_rad\", \"length\", \"width\"]\n",
        "        ]\n",
        "        self.controlled_vehicle = ControlledVehicle(\n",
        "            road=self.road,\n",
        "            position=[x, y],\n",
        "            heading=psi_rad,\n",
        "            speed=np.linalg.norm([vx, vy]),\n",
        "            enable_lane_change=True,\n",
        "            timer=np.random.choice([0, 5, 10, 15]),  # Change this as needed\n",
        "        )\n",
        "        self.road.vehicles.append(self.controlled_vehicle)\n",
        "\n",
        "        # Create non-controlled vehicles (other agents) from the dataset\n",
        "        other_vehicles_data = agent_data.loc[agent_data[\"interesting_agent\"] == 0]\n",
        "        for _, vehicle_data in other_vehicles_data.iterrows():\n",
        "            x, y, vx, vy, psi_rad, length, width = vehicle_data[[\"x\", \"y\", \"vx\", \"vy\", \"psi_rad\", \"length\", \"width\"]]\n",
        "            vehicle = IDMVehicle(\n",
        "                road=self.road,\n",
        "                position=[x, y],\n",
        "                speed=np.linalg.norm([vx, vy]),\n",
        "                heading=psi_rad,\n",
        "                length=length,\n",
        "                width=width,\n",
        "                enable_lane_change=True,\n",
        "                timer=np.random.choice([0, 5, 10, 15]),  # Change this as needed\n",
        "            )\n",
        "            self.road.vehicles.append(vehicle)\n",
        "\n",
        "    def _reward(self, action: Action) -> float:\n",
        "        \"\"\"\n",
        "        The reward is defined to foster driving at high speed, on the rightmost lanes, and to avoid collisions.\n",
        "        :param action: the last action performed\n",
        "        :return: the corresponding reward\n",
        "        \"\"\"\n",
        "        rewards = self._rewards(action)\n",
        "        reward = sum(self.config.get(name, 0) * reward for name, reward in rewards.items())\n",
        "        if self.config[\"normalize_reward\"]:\n",
        "            reward = utils.lmap(reward,\n",
        "                                [self.config[\"collision_reward\"],\n",
        "                                 self.config[\"high_speed_reward\"] + self.config[\"right_lane_reward\"]],\n",
        "                                [0, 1])\n",
        "        reward *= rewards['on_road_reward']\n",
        "        return reward\n",
        "\n",
        "    def _rewards(self, action: Action) -> Dict[Text, float]:\n",
        "        neighbours = self.road.network.all_side_lanes(self.controlled_vehicle.lane_index)\n",
        "        lane = self.controlled_vehicle.target_lane_index[2]\n",
        "        # Use forward speed rather than speed, see https://github.com/eleurent/highway-env/issues/268\n",
        "        forward_speed = self.controlled_vehicle.speed * np.cos(self.controlled_vehicle.heading)\n",
        "        scaled_speed = utils.lmap(forward_speed, self.config[\"reward_speed_range\"], [0, 1])\n",
        "        return {\n",
        "            \"collision_reward\": float(self.controlled_vehicle.crashed),\n",
        "            \"right_lane_reward\": lane / max(len(neighbours) - 1, 1),\n",
        "            \"high_speed_reward\": np.clip(scaled_speed, 0, 1),\n",
        "            \"on_road_reward\": float(self.controlled_vehicle.on_road)\n",
        "        }\n",
        "\n",
        "\n",
        "class Net(nn.Module):\n",
        "    def __init__(self, state_dim, action_dim):\n",
        "        super(Net, self).__init__()\n",
        "        hidden_nodes1 = 128\n",
        "        hidden_nodes2 = 64\n",
        "        self.fc1 = nn.Linear(state_dim, hidden_nodes1)\n",
        "        self.fc2 = nn.Linear(hidden_nodes1, hidden_nodes2)\n",
        "        self.fc3 = nn.Linear(hidden_nodes2, action_dim)\n",
        "\n",
        "    def forward(self, state):\n",
        "        x = F.relu(self.fc1(state))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        out = self.fc3(x)\n",
        "        return out\n",
        "\n",
        "\n",
        "class DOUBLEDQN_CNN(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        env,\n",
        "        state_dim,\n",
        "        action_dim,\n",
        "        lr=0.001,\n",
        "        gamma=0.99,\n",
        "        batch_size=5,\n",
        "        timestamp=\"\",\n",
        "    ):\n",
        "        \"\"\"\n",
        "        :param env: object, a gym environment\n",
        "        :param state_dim: int, size of state space\n",
        "        :param action_dim: int, size of action space\n",
        "        :param lr: float, learning rate\n",
        "        :param gamma: float, discount factor\n",
        "        :param batch_size: int, batch size for training\n",
        "        \"\"\"\n",
        "        super(DOUBLEDQN_CNN, self).__init__()\n",
        "\n",
        "        self.env = env\n",
        "        self.timestamp = timestamp\n",
        "\n",
        "        self.test_env = copy.deepcopy(env)  # for evaluation purpose\n",
        "        self.state_dim = state_dim\n",
        "        self.action_dim = action_dim\n",
        "        self.gamma = gamma\n",
        "        self.batch_size = batch_size\n",
        "        self.learn_step_counter = 0\n",
        "\n",
        "        self.target_net = Net(state_dim, action_dim).to(device)\n",
        "        self.estimate_net = Net(state_dim, action_dim).to(device)\n",
        "\n",
        "        self.optimizer = torch.optim.Adam(self.estimate_net.parameters(), lr=lr)\n",
        "\n",
        "    def update_target_networks(self):\n",
        "        \"\"\"\n",
        "        A function to update the target networks\n",
        "        \"\"\"\n",
        "        self.target_net.load_state_dict(self.estimate_net.state_dict())\n",
        "\n",
        "    def choose_action(self, state, epsilon=0.9):\n",
        "        state = np.expand_dims(state, axis=0)\n",
        "        state = np.expand_dims(state, axis=0)\n",
        "        state = torch.FloatTensor(state).to(device)\n",
        "        if np.random.randn() <= epsilon:\n",
        "            action_value = self.estimate_net(state)\n",
        "            action = torch.argmax(action_value).item()\n",
        "        else:\n",
        "            action = np.random.randint(0, self.action_dim)\n",
        "        return action\n",
        "\n",
        "    def preprocess_dataset(self, dataset):\n",
        "        \"\"\"\n",
        "        Preprocess the dataset to make it compatible with the highway_env\n",
        "\n",
        "        :param dataset: pandas DataFrame, input dataset\n",
        "        :return: numpy array, preprocessed dataset\n",
        "        \"\"\"\n",
        "        # Extract relevant columns\n",
        "        PIXEL_SCALE = 4\n",
        "        dataset = dataset[\n",
        "            [\n",
        "                \"frame_id\",\n",
        "                \"timestamp_ms\",\n",
        "                \"x\",\n",
        "                \"y\",\n",
        "                \"vx\",\n",
        "                \"vy\",\n",
        "                \"psi_rad\",\n",
        "                \"length\",\n",
        "                \"width\",\n",
        "                \"agent_type\",\n",
        "                \"interesting_agent\",\n",
        "            ]\n",
        "        ]\n",
        "\n",
        "        # Convert positions from meters to pixels\n",
        "        dataset[\"x\"] = dataset[\"x\"] * PIXEL_SCALE\n",
        "        dataset[\"y\"] = dataset[\"y\"] * PIXEL_SCALE\n",
        "\n",
        "        # Normalize positions, velocities, yaw angle, length, and width\n",
        "        dataset[\"x\"] = (dataset[\"x\"] - dataset[\"x\"].mean()) / dataset[\"x\"].std()\n",
        "        dataset[\"y\"] = (dataset[\"y\"] - dataset[\"y\"].mean()) / dataset[\"y\"].std()\n",
        "        dataset[\"vx\"] = (dataset[\"vx\"] - dataset[\"vx\"].mean()) / dataset[\"vx\"].std()\n",
        "        dataset[\"vy\"] = (dataset[\"vy\"] - dataset[\"vy\"].mean()) / dataset[\"vy\"].std()\n",
        "        dataset[\"psi_rad\"] = (\n",
        "            dataset[\"psi_rad\"] - dataset[\"psi_rad\"].mean()\n",
        "        ) / dataset[\"psi_rad\"].std()\n",
        "        dataset[\"length\"] = (\n",
        "            dataset[\"length\"] - dataset[\"length\"].mean()\n",
        "        ) / dataset[\"length\"].std()\n",
        "        dataset[\"width\"] = (dataset[\"width\"] - dataset[\"width\"].mean()) / dataset[\n",
        "            \"width\"\n",
        "        ].std()\n",
        "        # Encode agent types using one-hot encoding\n",
        "        agent_types = pd.get_dummies(dataset[\"agent_type\"], prefix=\"agent_type\")\n",
        "        dataset = pd.concat([dataset, agent_types], axis=1)\n",
        "\n",
        "        dataset.drop(columns=[\"agent_type\"], inplace=True)\n",
        "\n",
        "        return dataset.to_numpy()\n",
        "\n",
        "    def train(self, num_epochs, dataset):\n",
        "        dataset = self.preprocess_dataset(dataset)\n",
        "\n",
        "        count_list = []\n",
        "        loss_list = []\n",
        "        total_reward_list = []\n",
        "        avg_reward_list = []\n",
        "        epoch_reward = 0\n",
        "\n",
        "        state = self.env._reset()  # Reset the environment once at the beginning of training\n",
        "        for epoch in range(int(num_epochs)):\n",
        "            done = False\n",
        "            avg_loss = 0\n",
        "            step = 0\n",
        "            while not done:\n",
        "                step += 1\n",
        "                action = self.choose_action(state)\n",
        "                state_next, reward, done, _ = self.env.step(action)\n",
        "                exp = {\n",
        "                    \"state\": state,\n",
        "                    \"action\": action,\n",
        "                    \"reward\": reward,\n",
        "                    \"state_next\": state_next,\n",
        "                    \"done\": done,\n",
        "                }\n",
        "                # Update the replay buffer with the experience\n",
        "                self.ReplayBuffer.buffer_add(exp)\n",
        "                state = state_next\n",
        "\n",
        "                # sample random batch from replay memory\n",
        "                exp_batch = self.ReplayBuffer.buffer_sample(self.batch_size)\n",
        "\n",
        "                # extract batch data\n",
        "                state_batch = torch.FloatTensor([exp[\"state\"] for exp in exp_batch])\n",
        "                action_batch = torch.LongTensor([exp[\"action\"] for exp in exp_batch])\n",
        "                reward_batch = torch.FloatTensor([exp[\"reward\"] for exp in exp_batch])\n",
        "                state_next_batch = torch.FloatTensor(\n",
        "                    [exp[\"state_next\"] for exp in exp_batch]\n",
        "                )\n",
        "                done_batch = torch.FloatTensor([1 - exp[\"done\"] for exp in exp_batch])\n",
        "\n",
        "                # reshape\n",
        "                state_batch = state_batch.to(device).unsqueeze(1)\n",
        "                action_batch = action_batch.to(device).unsqueeze(1)\n",
        "                reward_batch = reward_batch.to(device).unsqueeze(1)\n",
        "                state_next_batch = state_next_batch.to(device).unsqueeze(1)\n",
        "                done_batch = done_batch.to(device).unsqueeze(1)\n",
        "\n",
        "                # get estimate Q value\n",
        "                estimate_Q = self.estimate_net(state_batch).gather(1, action_batch)\n",
        "\n",
        "                # get target Q value\n",
        "                max_action_idx = self.estimate_net(state_next_batch).detach().argmax(1)\n",
        "                target_Q = reward_batch + done_batch * self.gamma * self.target_net(\n",
        "                    state_next_batch\n",
        "                ).gather(1, max_action_idx.unsqueeze(1))\n",
        "\n",
        "                # compute mse loss\n",
        "                loss = F.mse_loss(estimate_Q, target_Q)\n",
        "                avg_loss += loss.item()\n",
        "\n",
        "                # update network\n",
        "                self.optimizer.zero_grad()\n",
        "                loss.backward()\n",
        "                self.optimizer.step()\n",
        "\n",
        "                # update target network\n",
        "                if self.learn_step_counter % 100 == 0:\n",
        "                    self.update_target_networks()\n",
        "                self.learn_step_counter += 1\n",
        "\n",
        "            reward, count = self.eval()\n",
        "            epoch_reward += reward\n",
        "\n",
        "            # save\n",
        "            period = 40\n",
        "            if epoch % period == 0:\n",
        "                # log\n",
        "                avg_loss /= step\n",
        "                epoch_reward /= period\n",
        "                avg_reward_list.append(epoch_reward)\n",
        "                loss_list.append(avg_loss)\n",
        "\n",
        "                print(\n",
        "                    \"\\nepoch: [{}/{}], \\tavg loss: {:.4f}, \\tavg reward: {:.3f}, \\tsteps: {}\".format(\n",
        "                        epoch + 1, num_epochs, avg_loss, epoch_reward, count\n",
        "                    )\n",
        "                )\n",
        "\n",
        "                epoch_reward = 0\n",
        "                # create a new directory for saving\n",
        "                try:\n",
        "                    os.makedirs(self.timestamp)\n",
        "                except OSError:\n",
        "                    pass\n",
        "                np.save(self.timestamp + \"/double_dqn_cnn_loss.npy\", loss_list)\n",
        "                np.save(\n",
        "                    self.timestamp + \"/double_dqn_cnn_avg_reward.npy\", avg_reward_list\n",
        "                )\n",
        "                torch.save(\n",
        "                    self.estimate_net.state_dict(),\n",
        "                    self.timestamp + \"/double_dqn_cnn.pkl\",\n",
        "                )\n",
        "\n",
        "        self.env.close()\n",
        "        return loss_list, avg_reward_list\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Read the dataset from the CSV file\n",
        "    dataset = pd.read_csv(\"/content/DR_LaneChange_ET0_obs.csv\")\n",
        "\n",
        "    # Timestamp for saving\n",
        "    named_tuple = time.localtime()  # get struct_time\n",
        "    time_string = time.strftime(\n",
        "        \"%m%d_%H_%M\", named_tuple\n",
        "    )\n",
        "\n",
        "    # Create an instance of the HighwayEnv with the dataset\n",
        "    # highway_env = HighwayEnv(dataset=dataset, config={})\n",
        "    highway_env = HighwayEnv(dataset, config={\"lanes_count\": 4})  # Adjust the 'lanes_count' as needed\n",
        "\n",
        "\n",
        "    # Create an instance of the DOUBLEDQN_CNN agent with the HighwayEnv\n",
        "    double_dqn_cnn_agent = DOUBLEDQN_CNN(\n",
        "        env=highway_env,\n",
        "        state_dim=5,  # Adjust state dimension according to your dataset\n",
        "        action_dim=4,  # Adjust action dimension according to your problem\n",
        "        lr=0.001,\n",
        "        gamma=0.99,\n",
        "        batch_size=64,\n",
        "        timestamp=time_string,\n",
        "    )\n",
        "\n",
        "    # Train the agent\n",
        "    iterations = 4000\n",
        "    avg_loss, avg_reward_list = double_dqn_cnn_agent.train(iterations, dataset)\n",
        "    np.save(time_string + \"/double_dqn_cnn_loss.npy\", avg_loss)\n",
        "    np.save(time_string + \"/double_dqn_cnn_avg_reward.npy\", avg_reward_list)\n",
        "\n",
        "    # Save the DQN network\n",
        "    torch.save(\n",
        "        double_dqn_cnn_agent.estimate_net.state_dict(),\n",
        "        time_string + \"/double_dqn_cnn.pkl\",\n",
        "    )\n",
        "\n",
        "    # Plot the training progress\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(avg_loss)\n",
        "    plt.grid()\n",
        "    plt.title(\"Double DQN Loss\")\n",
        "    plt.xlabel(\"Epochs\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "    plt.savefig(\"double_dqn_loss.png\", dpi=150)\n",
        "    plt.show()\n",
        "\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(avg_reward_list)\n",
        "    plt.grid()\n",
        "    plt.title(\"Double DQN Training Reward\")\n",
        "    plt.xlabel(\"*40 Epochs\")\n",
        "    plt.ylabel(\"Reward\")\n",
        "    plt.savefig(time_string + \"/double_dqn_cnn_train_reward.png\", dpi=150)\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "8GxUhorkfnlH",
        "outputId": "50f577d6-97e8-4448-97ce-16e0d676df49"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-13-7ca630b88232>:267: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  dataset[\"x\"] = dataset[\"x\"] * PIXEL_SCALE\n",
            "<ipython-input-13-7ca630b88232>:268: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  dataset[\"y\"] = dataset[\"y\"] * PIXEL_SCALE\n",
            "<ipython-input-13-7ca630b88232>:271: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  dataset[\"x\"] = (dataset[\"x\"] - dataset[\"x\"].mean()) / dataset[\"x\"].std()\n",
            "<ipython-input-13-7ca630b88232>:272: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  dataset[\"y\"] = (dataset[\"y\"] - dataset[\"y\"].mean()) / dataset[\"y\"].std()\n",
            "<ipython-input-13-7ca630b88232>:273: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  dataset[\"vx\"] = (dataset[\"vx\"] - dataset[\"vx\"].mean()) / dataset[\"vx\"].std()\n",
            "<ipython-input-13-7ca630b88232>:274: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  dataset[\"vy\"] = (dataset[\"vy\"] - dataset[\"vy\"].mean()) / dataset[\"vy\"].std()\n",
            "<ipython-input-13-7ca630b88232>:275: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  dataset[\"psi_rad\"] = (\n",
            "<ipython-input-13-7ca630b88232>:278: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  dataset[\"length\"] = (\n",
            "<ipython-input-13-7ca630b88232>:281: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  dataset[\"width\"] = (dataset[\"width\"] - dataset[\"width\"].mean()) / dataset[\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-7ca630b88232>\u001b[0m in \u001b[0;36m<cell line: 400>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    426\u001b[0m     \u001b[0;31m# Train the agent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    427\u001b[0m     \u001b[0miterations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m4000\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 428\u001b[0;31m     \u001b[0mavg_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mavg_reward_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdouble_dqn_cnn_agent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    429\u001b[0m     \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime_string\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"/double_dqn_cnn_loss.npy\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mavg_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    430\u001b[0m     \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime_string\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"/double_dqn_cnn_avg_reward.npy\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mavg_reward_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-13-7ca630b88232>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, num_epochs, dataset)\u001b[0m\n\u001b[1;32m    299\u001b[0m         \u001b[0mepoch_reward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    300\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 301\u001b[0;31m         \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Reset the environment once at the beginning of training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    302\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    303\u001b[0m             \u001b[0mdone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-13-7ca630b88232>\u001b[0m in \u001b[0;36m_reset\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     67\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mObservation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_road\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_vehicles\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m         state = np.array([\n\u001b[1;32m     71\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrolled_vehicle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mposition\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-13-7ca630b88232>\u001b[0m in \u001b[0;36m_create_vehicles\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    114\u001b[0m             \u001b[0;34m[\u001b[0m\u001b[0;34m\"x\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"y\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"vx\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"vy\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"psi_rad\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"length\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"width\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         ]\n\u001b[0;32m--> 116\u001b[0;31m         self.controlled_vehicle = ControlledVehicle(\n\u001b[0m\u001b[1;32m    117\u001b[0m             \u001b[0mroad\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroad\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m             \u001b[0mposition\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: ControlledVehicle.__init__() got an unexpected keyword argument 'enable_lane_change'"
          ]
        }
      ]
    }
  ]
}